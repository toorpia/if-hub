#!/usr/bin/env python3
"""
PI Batch Ingester - ç‹¬ç«‹ã—ãŸãƒãƒƒãƒå‡¦ç†ãƒ„ãƒ¼ãƒ«

PI Systemã‹ã‚‰ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§å–å¾—ã™ã‚‹ãƒãƒƒãƒå‡¦ç†ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
IF-Hub PI-Ingesterã‚µãƒ¼ãƒ“ã‚¹ã¨ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã—ã€æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ã§ãã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
    python pi-batch-ingester.py --config ../configs/equipments/7th-untan/config.yaml --start "2025-01-01" --end "2025-01-31"
"""

import os
import sys
import csv
import json
import re
import time
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.parse import urlencode
from urllib.error import URLError, HTTPError
from typing import Dict, Any, Optional, List


class PIBatchIngester:
    """PI System ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, equipment_config_path: str, pi_host: str, pi_port: int, 
                 timeout: int = 30000, max_retries: int = 3, retry_interval: int = 5000,
                 metadata_dir: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            equipment_config_path: è¨­å‚™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            pi_host: PI-API-Serverã®ãƒ›ã‚¹ãƒˆ
            pi_port: PI-API-Serverã®ãƒãƒ¼ãƒˆ
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒŸãƒªç§’ï¼‰
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            retry_interval: ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
            metadata_dir: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.equipment_config_path = Path(equipment_config_path)
        self.equipment_config = self._load_equipment_config()
        self.pi_config = {
            'host': pi_host,
            'port': pi_port,
            'timeout': timeout,
            'max_retries': max_retries,
            'retry_interval': retry_interval
        }
        self.equipment_name = self._extract_equipment_name()
        self.metadata_dir = Path(metadata_dir) if metadata_dir else None
        
    def _parse_simple_yaml(self, content: str) -> Dict[str, Any]:
        """ç°¡å˜ãªYAMLãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨ï¼‰"""
        result = {}
        lines = content.split('\n')
        stack = [result]
        indent_stack = [-1]
        
        for line in lines:
            line = line.rstrip()
            if not line or line.strip().startswith('#'):
                continue
                
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
            indent = len(line) - len(line.lstrip())
            stripped = line.strip()
            
            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã«åŸºã¥ã„ã¦ã‚¹ã‚¿ãƒƒã‚¯ã‚’èª¿æ•´
            while len(indent_stack) > 1 and indent <= indent_stack[-1]:
                stack.pop()
                indent_stack.pop()
            
            current_dict = stack[-1]
            
            if ':' in stripped and not stripped.startswith('- '):
                # ã‚­ãƒ¼: å€¤ã®ãƒšã‚¢
                key, value = stripped.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ï¼ˆ#ä»¥é™ã‚’é™¤å»ï¼‰
                if '#' in value:
                    value = value.split('#')[0].strip()
                
                # å€¤ã®å‹å¤‰æ›
                if value == 'true':
                    value = True
                elif value == 'false':
                    value = False
                elif value.startswith('"') and value.endswith('"'):
                    value = value[1:-1]
                elif value and value.lstrip('-').isdigit():
                    value = int(value)
                elif value and '.' in value:
                    # æµ®å‹•å°æ•°ç‚¹æ•°ã®ãƒã‚§ãƒƒã‚¯ã‚’æ”¹å–„
                    try:
                        value = float(value)
                    except ValueError:
                        pass  # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒ
                elif not value or value == '':
                    # ç©ºã®å€¤ã®å ´åˆã€ãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã‚’ä½œæˆ
                    value = {}
                    stack.append(value)
                    indent_stack.append(indent)
                
                current_dict[key] = value
                
            elif stripped.startswith('- '):
                # ãƒªã‚¹ãƒˆé …ç›®
                item = stripped[2:].strip()
                if item.startswith('"') and item.endswith('"'):
                    item = item[1:-1]
                
                # ç›´å‰ã®ã‚­ãƒ¼ãŒãƒªã‚¹ãƒˆã§ãªã„å ´åˆã€ãƒªã‚¹ãƒˆã«å¤‰æ›
                if stack and len(stack) >= 2:
                    parent_dict = stack[-2]
                    parent_keys = list(parent_dict.keys())
                    if parent_keys:
                        parent_key = parent_keys[-1]
                        if not isinstance(parent_dict[parent_key], list):
                            parent_dict[parent_key] = []
                        parent_dict[parent_key].append(item)
        
        return result
    
    def _load_equipment_config(self) -> Dict[str, Any]:
        """è¨­å‚™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.equipment_config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            config = self._parse_simple_yaml(content)
            
            # PIé€£æºãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            if not config.get('pi_integration', {}).get('enabled', False):
                raise ValueError(f"PI integration is disabled in {self.equipment_config_path}")
                
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"Equipment config file not found: {self.equipment_config_path}")
        except Exception as e:
            raise ValueError(f"Invalid YAML in equipment config: {e}")
    
    def _extract_equipment_name(self) -> str:
        """è¨­å‚™åã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰æŠ½å‡º"""
        # è¨­å‚™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è¨­å‚™åã‚’æŠ½å‡ºï¼ˆæ‹¡å¼µå­ã‚’é™¤ãï¼‰
        return self.equipment_config_path.stem
    
    def format_date_for_pi(self, date: datetime) -> str:
        """æ—¥æ™‚ã‚’PI-APIå½¢å¼ï¼ˆyyyyMMddHHmmssï¼‰ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        return date.strftime("%Y%m%d%H%M%S")
    
    def parse_datetime(self, date_str: str) -> datetime:
        """æ—¥æ™‚æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        # è¤‡æ•°ã®å½¢å¼ã‚’ã‚µãƒãƒ¼ãƒˆ
        formats = [
            "%Y-%m-%d",
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%Y-%m-%dT%H:%M",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        raise ValueError(f"Invalid datetime format: {date_str}")
    
    def get_source_tags(self) -> List[str]:
        """è¨­å‚™è¨­å®šã‹ã‚‰PIã‚¿ã‚°ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        source_tags = self.equipment_config.get('basemap', {}).get('source_tags', [])
        if not source_tags:
            raise ValueError("No source_tags found in equipment config")
        return source_tags
    
    def fetch_data(self, start_date: datetime, end_date: datetime) -> str:
        """PI-APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        # PI-APIè¨­å®š
        base_url = f"http://{self.pi_config['host']}:{self.pi_config['port']}"
        timeout = self.pi_config['timeout'] / 1000  # ãƒŸãƒªç§’ã‚’ç§’ã«å¤‰æ›
        max_retries = self.pi_config['max_retries']
        retry_interval = self.pi_config['retry_interval'] / 1000  # ãƒŸãƒªç§’ã‚’ç§’ã«å¤‰æ›
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        source_tags = self.get_source_tags()
        params = {
            'TagNames': ','.join(source_tags),
            'StartDate': self.format_date_for_pi(start_date),
            'EndDate': self.format_date_for_pi(end_date)
        }
        
        print(f"ğŸ”„ PI-API Request:")
        print(f"   URL: {base_url}/PIData")
        print(f"   TagNames: {params['TagNames']}")
        print(f"   StartDate: {params['StartDate']}")
        print(f"   EndDate: {params['EndDate']}")
        
        # ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
        for attempt in range(1, max_retries + 1):
            try:
                print(f"   Attempt {attempt}/{max_retries}...")
                
                # TagNamesã®ã‚«ãƒ³ãƒã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã›ãšã€ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                tag_names = params['TagNames']
                other_params = {k: v for k, v in params.items() if k != 'TagNames'}
                other_encoded = urlencode(other_params)
                url_with_params = f"{base_url}/PIData/?TagNames={tag_names}&{other_encoded}"
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
                req = Request(url_with_params)
                req.add_header('Accept', 'text/csv')
                
                # HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
                with urlopen(req, timeout=timeout) as response:
                    response_data = response.read().decode('utf-8')
                    
                    lines = response_data.strip().split('\n')
                    data_rows = len(lines) - 1 if lines and lines[0].startswith('Timestamp') else len(lines)
                    print(f"âœ… PI-API fetch successful: {data_rows} data rows")
                    return response_data
                    
            except (URLError, HTTPError) as e:
                error_msg = str(e)
                if hasattr(e, 'code'):
                    error_msg = f"HTTP {e.code}: {error_msg}"
                print(f"âŒ Attempt {attempt} failed: {error_msg}")
                
                if attempt == max_retries:
                    raise Exception(f"Failed after {max_retries} attempts: {error_msg}")
                
                if attempt < max_retries:
                    print(f"â³ Retrying in {retry_interval}s...")
                    time.sleep(retry_interval)
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ Attempt {attempt} failed: {error_msg}")
                
                if attempt == max_retries:
                    raise Exception(f"Failed after {max_retries} attempts: {error_msg}")
                
                if attempt < max_retries:
                    print(f"â³ Retrying in {retry_interval}s...")
                    time.sleep(retry_interval)
    
    def extract_metadata_from_csv(self, csv_data: str) -> List[Dict[str, str]]:
        """PI-APIã‹ã‚‰å–å¾—ã—ãŸCSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        lines = csv_data.split('\n')
        
        if len(lines) < 3:
            raise ValueError('CSV data does not contain required metadata rows')
        
        # æœ€åˆã®3è¡Œã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        source_tags = [tag.strip() for tag in lines[0].split(',')]
        display_names = [name.strip() for name in lines[1].split(',')]
        units = [unit.strip() for unit in lines[2].split(',')]
        
        # æœ€åˆã®ã‚«ãƒ©ãƒ ï¼ˆdatetime/timestampï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
        metadata = []
        for i in range(1, len(source_tags)):
            if i < len(display_names) and i < len(units):
                if source_tags[i] and display_names[i] and units[i]:
                    metadata.append({
                        'source_tag': source_tags[i],
                        'display_name': display_names[i],
                        'unit': units[i]
                    })
        
        print(f"ğŸ“‹ Extracted metadata for {len(metadata)} tags")
        return metadata
    
    def load_existing_translations(self, language_code: str = 'ja') -> List[Dict[str, str]]:
        """æ—¢å­˜ã®translationsãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not self.metadata_dir:
            return []
        
        filename = f"translations_{language_code}.csv"
        file_path = self.metadata_dir / filename
        
        try:
            if not file_path.exists():
                print(f"Translations file does not exist: {file_path}")
                return []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.strip().split('\n')
            if len(lines) <= 1:
                return []  # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã¾ãŸã¯ç©ºãƒ•ã‚¡ã‚¤ãƒ«
            
            metadata = []
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã¿è¾¼ã¿
            for i in range(1, len(lines)):
                columns = [col.strip() for col in lines[i].split(',')]
                if len(columns) >= 3:
                    metadata.append({
                        'source_tag': columns[0],
                        'display_name': columns[1],
                        'unit': columns[2]
                    })
            
            print(f"ğŸ“‹ Loaded {len(metadata)} existing translations from {filename}")
            return metadata
            
        except Exception as e:
            print(f"âš ï¸  Failed to load existing translations from {file_path}: {e}")
            return []
    
    def save_metadata_to_translations(self, new_metadata: List[Dict[str, str]], language_code: str = 'ja') -> None:
        """æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ã®translationsãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.metadata_dir or not new_metadata:
            return
        
        filename = f"translations_{language_code}.csv"
        file_path = self.metadata_dir / filename
        
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            self.metadata_dir.mkdir(parents=True, exist_ok=True)
            
            # æ—¢å­˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            existing_metadata = self.load_existing_translations(language_code)
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼šæ—¢å­˜ã®source_tagã‚»ãƒƒãƒˆã‚’ä½œæˆ
            existing_tags = {meta['source_tag'] for meta in existing_metadata}
            
            # æ–°è¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
            new_entries = [meta for meta in new_metadata if meta['source_tag'] not in existing_tags]
            
            if not new_entries:
                print(f"ğŸ“‹ No new metadata to add to {filename}")
                return
            
            print(f"ğŸ“‹ Adding {len(new_entries)} new entries to {filename}")
            
            # å…¨ä½“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            all_metadata = existing_metadata + new_entries
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
            csv_lines = ['source_tag,display_name,unit']  # ãƒ˜ãƒƒãƒ€ãƒ¼
            csv_lines.extend([
                f"{meta['source_tag']},{meta['display_name']},{meta['unit']}"
                for meta in all_metadata
            ])
            
            csv_content = '\n'.join(csv_lines) + '\n'
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã€ãã®å¾Œã‚¢ãƒˆãƒŸãƒƒã‚¯ã«rename
            temp_path = file_path.with_suffix(f'.tmp.{int(time.time())}')
            with open(temp_path, 'w', encoding='utf-8') as f:
                f.write(csv_content)
            
            temp_path.rename(file_path)
            
            print(f"âœ… Successfully updated {filename} with {len(new_entries)} new entries")
            print(f"   File size: {file_path.stat().st_size} bytes, Total entries: {len(all_metadata)}")
            
        except Exception as e:
            print(f"âŒ Failed to update translations file {file_path}: {e}")
            raise
    
    def process_raw_csv_to_ifhub_format(self, csv_data: str) -> str:
        """PI-APIã‹ã‚‰å–å¾—ã—ãŸCSVãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã¦IF-HUBå½¢å¼ã«å¤‰æ›"""
        lines = csv_data.split('\n')
        
        if len(lines) < 4:
            raise ValueError('CSV data does not contain enough rows for processing')
        
        # 1è¡Œç›®ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ã¨4è¡Œç›®ä»¥é™ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿ã‚’ä¿æŒ
        processed_lines = [lines[0]] + lines[3:]
        
        # ç©ºè¡Œã‚’é™¤å»
        filtered_lines = [line for line in processed_lines if line.strip()]
        
        result = '\n'.join(filtered_lines)
        
        data_rows = len(filtered_lines) - 1 if filtered_lines and filtered_lines[0] else len(filtered_lines)
        print(f"ğŸ“‹ Processed CSV: {len(filtered_lines)} lines (removed metadata rows), {data_rows} data rows")
        
        return result
    
    def save_csv(self, data: str, output_path: str) -> None:
        """CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_file = Path(output_path)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(output_file, 'w', encoding='utf-8', newline='') as f:
                f.write(data)
            
            lines = data.strip().split('\n')
            data_rows = len(lines) - 1 if lines and lines[0].startswith('Timestamp') else len(lines)
            
            print(f"ğŸ’¾ CSV saved: {output_file}")
            print(f"   Data rows: {data_rows}")
            print(f"   File size: {output_file.stat().st_size} bytes")
            
        except Exception as e:
            raise Exception(f"Failed to save CSV file: {e}")
    
    def run_batch(self, start_date: datetime, end_date: datetime, output_path: Optional[str] = None) -> None:
        """ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›ãƒ‘ã‚¹
        if output_path is None:
            output_path = f"./{self.equipment_name}.csv"
        
        print(f"ğŸ­ PI Batch Ingester")
        print(f"   Equipment: {self.equipment_name}")
        print(f"   Period: {start_date} to {end_date}")
        print(f"   Output: {output_path}")
        print(f"   Tags: {len(self.get_source_tags())} tags")
        if self.metadata_dir:
            print(f"   Metadata dir: {self.metadata_dir}")
        print()
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        raw_csv_data = self.fetch_data(start_date, end_date)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆmetadata_dirãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        if self.metadata_dir:
            try:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                metadata = self.extract_metadata_from_csv(raw_csv_data)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’translationsãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                self.save_metadata_to_translations(metadata)
                
                # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦IF-Hubå½¢å¼ã«å¤‰æ›
                processed_csv_data = self.process_raw_csv_to_ifhub_format(raw_csv_data)
                
                # å‡¦ç†æ¸ˆã¿CSVã‚’ä¿å­˜
                self.save_csv(processed_csv_data, output_path)
                
            except Exception as e:
                print(f"âš ï¸  Metadata processing failed: {e}")
                print("   Saving raw CSV data instead...")
                self.save_csv(raw_csv_data, output_path)
        else:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãªã—ï¼ˆå¾“æ¥é€šã‚Šï¼‰
            self.save_csv(raw_csv_data, output_path)
        
        print()
        print("âœ… Batch processing completed successfully!")


def main():
    """
    PI Batch Ingester - PI Systemã‹ã‚‰ã®ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ„ãƒ¼ãƒ«
    
    æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’PI Systemã‹ã‚‰CSVå½¢å¼ã§å–å¾—ã—ã¾ã™ã€‚
    è¨­å‚™è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚°æƒ…å ±ã¨æ¥ç¶šè¨­å®šã‚’èª­ã¿è¾¼ã¿ã€ç‹¬ç«‹ã—ãŸãƒãƒƒãƒå‡¦ç†ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
    """
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(
        description='PI Batch Ingester - PI Systemã‹ã‚‰ã®ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python pi-batch-ingester.py -c equipment.yaml --host 10.255.234.21 --port 3011 -s "2025-01-01" -e "2025-01-31"
  python pi-batch-ingester.py -c equipment.yaml --host 10.255.234.21 --port 3011 -s "2025-01-01 08:00:00" -e "2025-01-01 17:00:00" -o "./backup/data.csv"

æ—¥æ™‚å½¢å¼:
  - 2025-01-01 (æ—¥ä»˜ã®ã¿)
  - 2025-01-01 12:30:00 (æ—¥ä»˜ã¨æ™‚åˆ»)
  - 2025-01-01T12:30:00 (ISOå½¢å¼)
        """
    )
    
    parser.add_argument('-c', '--config', required=True,
                       help='Equipment config file path (relative or absolute)')
    parser.add_argument('--host', required=True,
                       help='PI-API-Server hostname or IP address')
    parser.add_argument('--port', type=int, required=True,
                       help='PI-API-Server port number')
    parser.add_argument('-s', '--start', required=True,
                       help='Start datetime (e.g., "2025-01-01", "2025-01-01 12:00:00")')
    parser.add_argument('-e', '--end', required=True,
                       help='End datetime (e.g., "2025-01-31", "2025-01-31 23:59:59")')
    parser.add_argument('-o', '--output', default=None,
                       help='Output CSV file path (default: ./{equipment_name}.csv)')
    parser.add_argument('--metadata-dir', default=None,
                       help='Metadata output directory for tag translations (CSV format)')
    parser.add_argument('--timeout', type=int, default=30000,
                       help='Request timeout in milliseconds (default: 30000)')
    parser.add_argument('--retries', type=int, default=3,
                       help='Maximum number of retries (default: 3)')
    parser.add_argument('--retry-interval', type=int, default=5000,
                       help='Retry interval in milliseconds (default: 5000)')
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Enable verbose output')
    
    args = parser.parse_args()
    
    try:
        # PI Batch Ingesterã‚’åˆæœŸåŒ–
        ingester = PIBatchIngester(
            equipment_config_path=args.config,
            pi_host=args.host,
            pi_port=args.port,
            timeout=args.timeout,
            max_retries=args.retries,
            retry_interval=args.retry_interval,
            metadata_dir=args.metadata_dir
        )
        
        # æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
        start_date = ingester.parse_datetime(args.start)
        end_date = ingester.parse_datetime(args.end)
        
        # æ—¥æ™‚ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if start_date >= end_date:
            raise ValueError("Start date must be before end date")
        
        # æœŸé–“ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
        duration = end_date - start_date
        if duration.days > 365:
            print(f"âš ï¸  Large date range detected ({duration.days} days).")
            response = input("Continue? [y/N]: ").strip().lower()
            if response not in ['y', 'yes']:
                print("Operation cancelled.")
                sys.exit(0)
        
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        ingester.run_batch(start_date, end_date, args.output)
        
    except KeyboardInterrupt:
        print("\nâŒ Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Error: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
